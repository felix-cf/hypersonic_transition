{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################## IMPORTS ##########################\n",
    "\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re as RE\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.data as geom_data\n",
    "import torch_geometric.nn as geom_nn\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torch import Tensor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "from functools import reduce\n",
    "\n",
    "L.seed_everything(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:31: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:35: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:31: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:35: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\fouad\\AppData\\Local\\Temp\\ipykernel_25992\\1873567308.py:31: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  mach = (RE.findall(\"[0-9]+\\.[0-9]+\", line))[0]\n",
      "C:\\Users\\fouad\\AppData\\Local\\Temp\\ipykernel_25992\\1873567308.py:35: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  if len(RE.findall(\"[0-9]+\\.[0-9]+\", line)) > 3: # condition for dataline is usually more than a few numbers in that row\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# what do we need for one datapoint:\n",
    "# BL u/u_e\n",
    "# BL rho\n",
    "# BL T/T_e[0] = (T_w/T_e)\n",
    "# Re\n",
    "# omega = (2\\pi f) * (lscl/u_e)\n",
    "#  M_e\n",
    "\n",
    "# labelling: sigma (`s)\n",
    "\n",
    "########################## LOADING AND STORING DATA ##########################\n",
    "\n",
    "\n",
    "\n",
    "RESET = False\n",
    "'''\n",
    "Tpose = True : Use for returning a list of each data type\n",
    "Tpose = False: Use for printing out the data. \n",
    "'''\n",
    "def ndata_reader(file,tpose=True):\n",
    "    arrays = []\n",
    "    labels = []\n",
    "    looking = 0\n",
    "    with open(file, 'r') as a:\n",
    "        for line in a.readlines():\n",
    "            if looking > 1:\n",
    "                break\n",
    "            if len(RE.findall(\"Parameters used for normalizatons\", line)) > 0:\n",
    "                looking += 1\n",
    "            if len(RE.findall(\"#  Mach number       = \", line)) > 0:\n",
    "                mach = (RE.findall(\"[0-9]+\\.[0-9]+\", line))[0]\n",
    "            if line[:12] == \"variables = \" and len(labels)==0:\n",
    "                labels = line[12:].split(',')\n",
    "                labels = [l.strip() for l in labels]\n",
    "            if len(RE.findall(\"[0-9]+\\.[0-9]+\", line)) > 3: # condition for dataline is usually more than a few numbers in that row\n",
    "                arrays.append([float(s) for s in line.split()])\n",
    "\n",
    "    arrays = np.transpose(arrays) if tpose else arrays\n",
    "    return labels, arrays, mach\n",
    "\n",
    "def bl_input_reader(file,tpose=True):\n",
    "    arrays = []\n",
    "    with open(file, 'r') as a:\n",
    "        for line in a.readlines():\n",
    "            arrays.append([float(s) for s in line.split()])\n",
    "            \n",
    "    arrays = np.transpose(arrays) if tpose else arrays\n",
    "    return [\"eta\", \"rho\", \"u\", \"v\", \"T\"], arrays\n",
    "\n",
    "def all_pos(example):\n",
    "    # Grab one file to determine the positions of everything\n",
    "    labels, res, mach = ndata_reader(example)\n",
    "    res = np.array(res)\n",
    "    x_pos = labels.index('x')\n",
    "    sigma_pos = labels.index('`s')\n",
    "    freq_pos = labels.index('f')\n",
    "    Re_pos = labels.index('Re')\n",
    "    lscl_pos = labels.index('lscl')\n",
    "    ue_pos = labels.index('u_e')\n",
    "    return [sigma_pos, freq_pos, Re_pos, lscl_pos, ue_pos, mach, x_pos]\n",
    "\n",
    "# DATAPOINT: U, RHO, TWTE, RE, OMEGA, MACH, SIG (LABEL)\n",
    "\n",
    "# mode = \"second_mode\" or \"first_mode\"\n",
    "def write_data(root, write_file, mode=1):\n",
    "\n",
    "    mode = \"first_mode\" if mode==1 else \"second_mode\" if mode==2 else None\n",
    "    # Extract the positions of all important features from an example file\n",
    "    pos = all_pos(\"data\\\\constantRe.tar\\\\constantRe\\\\fp_M4.0_Tw_300K\\\\lst\\\\second_mode\\\\case_f_00100000\\\\nfact.dat\")\n",
    "    sigma_pos = pos[0]\n",
    "    freq_pos = pos[1]\n",
    "    Re_pos = pos[2]\n",
    "    lscl_pos = pos[3]\n",
    "    ue_pos = pos[4]\n",
    "    mach = pos[5]\n",
    "\n",
    "    print(\"Searching through data in \" + root)\n",
    "    mach_dirs = [\"\\\\\" + l for l in os.listdir(root)]\n",
    "\n",
    "    for m in tqdm(mach_dirs): # :1 because only first one\n",
    "        print(\"\\tLoading \" + m)\n",
    "\n",
    "        batch = []\n",
    "\n",
    "        # find the bl profile\n",
    "        _, bl_profile = bl_input_reader(root + m + \"\\\\mkBaseflow\\\\bl_input.dat\", tpose=True)\n",
    "\n",
    "        # Grab T_w/T_e which is the first value of T\n",
    "        TwTe = bl_profile[4][0]\n",
    "\n",
    "        # We also want to figure out what the boundary thickness is. Since the length scale changes for each output in a frequency result,\n",
    "        # we want to extract the boundary layer thickness that is nondimensionalized by lscl, which is the value of eta at which u converges \n",
    "        # to about 1. I will determine this value by identifying the point at which the difference between two eta values is < 1e-9 or so.\n",
    "        eta = bl_profile[0]\n",
    "        u_fine = bl_profile[2] # fine-grained u values to get accurate reading on boundary layer thickness\n",
    "        ind = 0\n",
    "\n",
    "        while(ind < len(u_fine)-1 and abs(u_fine[ind] - u_fine[ind+1]) > 1e-9): ind += 1\n",
    "        thickness_eta = eta[ind]\n",
    "\n",
    "        # Grab ~60 equidistant points of u and rho information from the boundary layer itself. We only care about the boundary layer so\n",
    "        # ignore all points after 'ind'\n",
    "        \n",
    "        # to get exactly 60 points each time, first find how much buffer we need from the number of points on the boundary layer to get a \n",
    "        # number of points divisible by 60\n",
    "        diff = (ind+1)%60\n",
    "        # find where the new ind must be set; this number must be divisible by 60\n",
    "        new_ind = ind + diff + 1 if (ind+diff+1)%60 == 0 else ind + 1 - diff\n",
    "        step = round(new_ind/60) # just in case the boundary layer has less than 1 point?\n",
    "        u = bl_profile[2][:new_ind:step]\n",
    "        rho = bl_profile[1][:new_ind:step]\n",
    "        \n",
    "        # now find all lst data from the corresponding mode\n",
    "        m_lst_mode = m + \"\\\\lst\\\\\" + mode\n",
    "        lst_2_dirs = [\"\\\\\" + l for l in os.listdir(root+m_lst_mode) if len(RE.findall(\"^case_f_[0-9]+$\", l)) > 0]\n",
    "        for ldir in lst_2_dirs: # :1 because only first one\n",
    "            # now look for the specific nfact.dat file in this file\n",
    "            #print(\"\\t\\tLoading \" + root+m_lst_mode+ldir)\n",
    "            _, ndata, mach = ndata_reader(root+m_lst_mode+ldir+\"\\\\nfact.dat\")\n",
    "            for line in range(len(ndata[0]) if len(ndata)>0 else 0):\n",
    "                \n",
    "                Re_l = ndata[Re_pos][line]\n",
    "                sig = ndata[sigma_pos][line]\n",
    "                freq = ndata[freq_pos][line]\n",
    "                lscl = ndata[lscl_pos][line]\n",
    "                u_e = ndata[ue_pos][line]\n",
    "\n",
    "                # Require a dimensional value for boundary layer thickness. To do this, we take the nondim eta value, eta = y/lscl,\n",
    "                # multiply it with lscl at this point, and have gotten a y value which defines the dim boundary layer thickness \n",
    "                thickness = thickness_eta * lscl\n",
    "\n",
    "                # Now we re-dim Re_l and dim omega with boundary layer thickness \n",
    "\n",
    "                omega = 2*np.pi*freq*thickness/u_e\n",
    "                Re = Re_l * thickness_eta # because thickness_eta = y/lscl, so it divides out the lscl and mults with the y val\n",
    "                sig = sig * thickness_eta # thickness_eta = y/lscl, so sig * lscl * y/lscl = sig * y\n",
    "                \n",
    "                datapoint = []\n",
    "                datapoint += list(u) \n",
    "                datapoint += list(rho) \n",
    "                datapoint += [TwTe, Re, omega, mach, sig]\n",
    "                #print(datapoint)\n",
    "                #print(len(datapoint))\n",
    "                # should be 60 + 60 + 5\n",
    "                \n",
    "                # add this to the batch\n",
    "                batch += [datapoint]\n",
    "\n",
    "\n",
    "        # convert the batch to a dataframe\n",
    "        batch = np.array(batch)\n",
    "        df = pd.DataFrame(batch)\n",
    "        df.to_csv(write_file, mode='a', index=False, header=False)\n",
    "\n",
    "\n",
    "\n",
    "root = \"data\\\\constantRe.tar\\\\constantRe\"\n",
    "write_file_root = \"data\\\\csv_files\\\\\"\n",
    "\n",
    "first_file = write_file_root + \"small_first.csv\"\n",
    "sec_file = write_file_root + \"small_second.csv\"\n",
    "# reset the csv file\n",
    "if(RESET):\n",
    "    if(os.path.isfile(first_file)):\n",
    "        os.remove(first_file)\n",
    "    write_data(root, first_file, mode=1) # write the data to the csv file\n",
    "\n",
    "    if(os.path.isfile(sec_file)):\n",
    "        os.remove(sec_file)\n",
    "    write_data(root, sec_file, mode=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## CREATING DATASETS AND DATALOADERS ##########################\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = [StandardScaler(), StandardScaler()]\n",
    "sig_scaler = [StandardScaler(), StandardScaler()]\n",
    "class ProfileDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file):\n",
    "        self.data = pd.read_csv(file, header=None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        res = self.data.iloc[idx]\n",
    "        res = np.array(res)\n",
    "        return res\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "crazy = False\n",
    "\n",
    "pfd = []\n",
    "train_pfd = []\n",
    "val_pfd = []\n",
    "test_pfd = []\n",
    "\n",
    "train_loader = []\n",
    "val_loader = []\n",
    "test_loader = []\n",
    "\n",
    "crazy_loader = []\n",
    "dum = []\n",
    "for ind in range(2):\n",
    "    pfd += [ProfileDataset(first_file if ind==0 else sec_file)]\n",
    "    dum += [sig_scaler[ind].fit_transform(pfd[ind][:,124:])]\n",
    "    pfd[ind] = scaler[ind].fit_transform(pfd[ind])\n",
    "\n",
    "    splits = [int(pfd[ind].__len__() * 0.8), int(pfd[ind].__len__()*0.9)] # 80 train, 10 val, 10 test\n",
    "\n",
    "    train_pfd += [pfd[ind][:splits[0]]]\n",
    "    val_pfd += [pfd[ind][splits[0]:splits[1]]]\n",
    "    test_pfd += [pfd[ind][splits[1]:]]\n",
    "\n",
    "    if crazy:\n",
    "        crazy_loader += [DataLoader(torch.tensor(pfd[ind]).float(), batch_size=BATCH_SIZE, shuffle=False, num_workers=7, persistent_workers=True)]\n",
    "    else:\n",
    "        train_loader += [DataLoader(torch.tensor(train_pfd[ind]).float(), batch_size=BATCH_SIZE, shuffle=True, num_workers=7, persistent_workers=True)]\n",
    "        val_loader += [DataLoader(torch.tensor(val_pfd[ind]).float(), batch_size=BATCH_SIZE, shuffle=True, num_workers=7, persistent_workers=True)]\n",
    "        test_loader += [DataLoader(torch.tensor(test_pfd[ind]).float(), batch_size=BATCH_SIZE, shuffle=True, num_workers=7, persistent_workers=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14237\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "pfd = ProfileDataset(first_file)\n",
    "print(pfd.__len__()) # first mode: 354166, second mode: 31087\n",
    "print(len(pfd.__getitem__(0))) # should be 125 = 60 u + 60 rho + 4 TwTe, Me, omega_e, Re_d + 1 sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 124])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0272],\n",
       "        [-0.0272],\n",
       "        [-0.0272]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################## PROFILE AND VARIABLE NETWORK ##########################\n",
    "\n",
    "\n",
    "\n",
    "class ProfileCNN_LatentFCN(nn.Module):\n",
    "    def __init__(self, c_in=2, output=8, hidden_c=[4,8,4], alpha=0.1, kernels=[3, 2], hidden_l=[4+8,96,96,96,96,96,96,1]):\n",
    "        super(ProfileCNN_LatentFCN, self).__init__()\n",
    "        \n",
    "        '''\n",
    "        ProfileConv: maps an input of u, rho, Tw/Te, Re, omega, and Me into a predicted sigma value\n",
    "        c_in - input channels of convolution, typically u and rho.  \n",
    "        output - latent vector size, which encodes all relevant information from the boundary profile\n",
    "        hidden_c - channels of all hidden convolutional and pooling layers.\n",
    "        alpha - LeakyReLU coefficient\n",
    "        kernels - kernels for convolution and for maxpooling.   \n",
    "        '''\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.nonlins = nn.ModuleList()\n",
    "        self.maxpools = nn.ModuleList()\n",
    "\n",
    "        self.channels = [c_in] + hidden_c\n",
    "        self.hidden_l = hidden_l\n",
    "\n",
    "        self.fcn = nn.ModuleList()\n",
    "        self.nonlins2 = nn.ModuleList()\n",
    "\n",
    "        # Define three convolutional, nonlin, and maxpool layers\n",
    "        for i in range(len(self.channels)-1):\n",
    "            self.convs.append(nn.Conv1d(in_channels=self.channels[i], out_channels=self.channels[i+1],\n",
    "                               kernel_size=kernels[0]))\n",
    "            self.nonlins.append(nn.LeakyReLU(alpha))\n",
    "            self.maxpools.append(nn.MaxPool1d(kernel_size=kernels[1], stride=kernels[1]))\n",
    "\n",
    "        # Then flatten the result and connect a linear layer to the output latent vectors\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.latent = nn.Linear(4*5, output) # The final dimension will be 4x5 and we do fcn to 8 latent vars # OR 4 FOR 4,8,8,4?\n",
    "\n",
    "        # Now we have 8 resultant latent parameters and 4 extra parameters that we pass through FCNs\n",
    "        for i in range(len(hidden_l)-1):\n",
    "            self.fcn.append(nn.Linear(hidden_l[i], hidden_l[i+1]))\n",
    "            self.nonlins2.append(nn.LeakyReLU(alpha))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Takes in input, containing u and rho as the first 60+60+4 params,\n",
    "        pass it through a CNN, then pass the resultant 8-parameter latent vector\n",
    "        and the four extra params into multiple FCNs to finally predict one value. \n",
    "        '''\n",
    "\n",
    "        # First split up the data into u, rho, etc\n",
    "        u = x[:, :60]\n",
    "        rho = x[:, 60:120]\n",
    "        u = u.unsqueeze(1)\n",
    "        rho = rho.unsqueeze(1)\n",
    "        y = torch.cat((u, rho), dim=1) # shape: (batch_size, 2, 60)\n",
    "        \n",
    "        TwTe = x[:, 120:121]\n",
    "        Re = x[:, 121:122]\n",
    "        omega = x[:, 122:123]\n",
    "        mach = x[:, 123:124]\n",
    "        periph = torch.cat((TwTe, Re, omega, mach), dim=1)\n",
    "\n",
    "        # pass through conv layers\n",
    "        for i in range(len(self.channels)-1):\n",
    "            y = self.convs[i](y)\n",
    "            y = self.nonlins[i](y)\n",
    "            y = self.maxpools[i](y)\n",
    "\n",
    "        # Now flatten and pass through fcn to get 8 latent variables\n",
    "        y = self.flatten(y) # shape: (batch_size, 4 * 5) \n",
    "        y = self.latent(y) # shape: (batch_size, 8)\n",
    "\n",
    "        y = torch.cat((y,periph), dim=1) # shape: (batch_size, 12)\n",
    "\n",
    "        # pass through final fcns\n",
    "        for i in range(len(self.hidden_l)-1):\n",
    "            y = self.nonlins2[i](y)\n",
    "            y = self.fcn[i](y)\n",
    "\n",
    "        # final shape: (batch_size, 1)     \n",
    "        return y\n",
    "    \n",
    "\n",
    "mod = ProfileCNN_LatentFCN()\n",
    "mod.parameters()\n",
    "ex = [[x for x in range(124)] for y in range(3)]\n",
    "ex = torch.tensor(np.array(ex)).float()\n",
    "print(ex.shape)\n",
    "mod.forward(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## AMPLIFICATION PREDICTOR ##########################\n",
    "\n",
    "\n",
    "\n",
    "class AmplificationPredictor(L.LightningModule):\n",
    "    def __init__(self, mode, **model_kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        if mode==1:\n",
    "            alpha = 0.1\n",
    "        if mode==2:\n",
    "            alpha = 0.2\n",
    "        self.model = ProfileCNN_LatentFCN(alpha=alpha, **model_kwargs)\n",
    "        #self.model = SimpleNN(**model_kwargs)\n",
    "\n",
    "        \n",
    "        self.split = 124\n",
    "        #self.split = 3\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x = self.model(data[:, :self.split])  # Assuming model outputs predicted values\n",
    "        return x\n",
    "\n",
    "    def compute_loss_and_metric(self, batch):\n",
    "        data, target = batch[:, :self.split], batch[:, self.split].float()  # Regression targets should be floats\n",
    "        predictions = self.forward(data).squeeze()\n",
    "        loss = torch.norm(predictions - target, p='fro') / torch.norm(target, p='fro') # Normalized root mean square error\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.compute_loss_and_metric(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.compute_loss_and_metric(batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.compute_loss_and_metric(batch)\n",
    "        self.log(\"test_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### TRAINER CLASS ##########################\n",
    "\n",
    "\n",
    "\n",
    "CHECKPOINT_PATH = \"checkpoints\\\\\"\n",
    "\n",
    "def train_graph_classifier(model_name, train_loader, val_loader, test_loader, mode, **model_kwargs):\n",
    "    L.seed_everything(42)\n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = L.Trainer(\n",
    "        default_root_dir=root_dir,\n",
    "        callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"val_loss\")],\n",
    "        accelerator=\"auto\",\n",
    "        devices=\"auto\",\n",
    "        strategy=\"auto\",\n",
    "        max_epochs=200,\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "    trainer.logger._default_hp_metric = None\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    #pretrained_filename = os.path.join(CHECKPOINT_PATH, \"%s.ckpt\" % (model_name + str(mode)))\n",
    "    L.seed_everything(42)\n",
    "    model = AmplificationPredictor(mode=mode, **model_kwargs)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    #model = AmplificationPredictor.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "    #trainer.save_checkpoint(pretrained_filename)\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    train_result = trainer.test(model, dataloaders=train_loader, verbose=False)\n",
    "    test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0][\"test_loss\"], \"train\": train_result[0][\"test_loss\"]}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\fouad\\anaconda3\\envs\\gnn_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                 | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model | ProfileCNN_LatentFCN | 48.3 K | train\n",
      "-------------------------------------------------------\n",
      "48.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "48.3 K    Total params\n",
      "0.193     Total estimated model params size (MB)\n",
      "c:\\Users\\fouad\\anaconda3\\envs\\gnn_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\fouad\\anaconda3\\envs\\gnn_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (45) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\fouad\\anaconda3\\envs\\gnn_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- FIRST MODE ---\n",
      "Train performance: 1.29%\n",
      "Test performance:  26.62%\n"
     ]
    }
   ],
   "source": [
    "########################## FIRST MODE TRAIN ##########################\n",
    "\n",
    "model, result = train_graph_classifier(model_name=\"AmplificationPredictor\",\n",
    "                                    train_loader = train_loader[0],\n",
    "                                    val_loader = val_loader[0],\n",
    "                                    test_loader = test_loader[0],\n",
    "                                    mode = 1)\n",
    "\n",
    "# run this to make sure we have the right model \n",
    "print(\" --- FIRST MODE ---\")\n",
    "print(\"Train performance: %4.2f%%\" % (100.0 * result[\"train\"]))\n",
    "print(\"Test performance:  %4.2f%%\" % (100.0 * result[\"test\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fouad\\AppData\\Local\\Temp\\ipykernel_25992\\493736278.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"first_mode.pth\")\n"
     ]
    }
   ],
   "source": [
    "########################## FIRST MODE SAVE/LOAD ##########################\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    if model.training:\n",
    "        model.eval()\n",
    "        print(\"Model set to eval mode.\")\n",
    "    # To save:\n",
    "    torch.save(model, \"first_mode.pth\")\n",
    "except:\n",
    "    # To load:\n",
    "    model = torch.load(\"first_mode.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## FIRST MODE PREDICTIONS #########################\n",
    "\n",
    "\n",
    "allBLs = \"allBLs\"\n",
    "root = \"allBLs\\\\\"\n",
    "\n",
    "mach_dirs = [l for l in os.listdir(root)]\n",
    "machs = [x*0.25 + 4 for x in range(13)]\n",
    "\n",
    "P0 = 341700.0\n",
    "gam = 1.4\n",
    "T1 = 63.9231\n",
    "R = 287.15\n",
    "Tw = 300.0\n",
    "\n",
    "# Now we want to loop through a list of frequencies and Reynolds numbers for each one of these profiles \n",
    "#freqs = [x*2500 + 20000 for x in range(100)] # 20,000 - 200,000 f\n",
    "#freqs = [x*1000 + 20000 for x in range(100)]\n",
    "#x_vals = [round(0.005*x + 0.001, 2) for x in range(100)] # 0.01 - 1.00 x\n",
    "#x_vals = [round(0.004*x + 0.1, 3) for x in range(100)]\n",
    "\n",
    "#PREDICTION!!!!\n",
    "\n",
    "x_vals = [round(0.02*x + 0.1, 3) for x in range(100)]\n",
    "freqs = [x*5000 + 20000 for x in range(100)]\n",
    "\n",
    "res = []\n",
    "\n",
    "for ind in range(len(mach_dirs)):\n",
    "    m = mach_dirs[ind]\n",
    "    mach = machs[ind]\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    # find the bl profile\n",
    "    _, bl_profile = bl_input_reader(root + m + \"\\\\bl_input.dat\", tpose=True)\n",
    "\n",
    "    # Grab T_w/T_e which is the first value of T\n",
    "    TwTe = bl_profile[4][0]\n",
    "    Te = Tw/TwTe\n",
    "    u_e = mach * np.sqrt(gam*R*Te)\n",
    "\n",
    "    # We also want to figure out what the boundary thickness is. Since the length scale changes for each output in a frequency result,\n",
    "    # we want to extract the boundary layer thickness that is nondimensionalized by lscl, which is the value of eta at which u converges \n",
    "    # to about 1. I will determine this value by identifying the point at which the difference between two eta values is < 1e-9 or so.\n",
    "    eta = bl_profile[0]\n",
    "    u_fine = bl_profile[2] # fine-grained u values to get accurate reading on boundary layer thickness\n",
    "    ind = 0\n",
    "\n",
    "    while(ind < len(u_fine)-1 and abs(u_fine[ind] - u_fine[ind+1]) > 1e-9): ind += 1\n",
    "    thickness_eta = eta[ind]\n",
    "\n",
    "    # Grab ~60 equidistant points of u and rho information from the boundary layer itself. We only care about the boundary layer so\n",
    "    # ignore all points after 'ind'\n",
    "    \n",
    "    # to get exactly 60 points each time, first find how much buffer we need from the number of points on the boundary layer to get a \n",
    "    # number of points divisible by 60\n",
    "    diff = (ind+1)%60\n",
    "    # find where the new ind must be set; this number must be divisible by 60\n",
    "    new_ind = ind + diff + 1 if (ind+diff+1)%60 == 0 else ind + 1 - diff\n",
    "    step = round(new_ind/60) # just in case the boundary layer has less than 1 point?\n",
    "    u = bl_profile[2][:new_ind:step]\n",
    "    rho = bl_profile[1][:new_ind:step]\n",
    "    \n",
    "\n",
    "    arg = 1+0.5*(gam-1.)*mach**2.\n",
    "    P1 = P0*(arg)**(-gam/(gam-1.))\n",
    "    re  = P1/(R*T1)\n",
    "    res_m = []\n",
    "    mu_e = 1.45151376745308e-06*Te**1.5e0/(Te+110.4e0)\n",
    "\n",
    "    for f in freqs:\n",
    "\n",
    "        batch = []\n",
    "        for x in x_vals: \n",
    "\n",
    "            # now we must determine our Re\n",
    "            # first determine the lscl\n",
    "            lscl = np.sqrt((mu_e*x)/(re*u_e))\n",
    "            # determine thickness\n",
    "            thickness = thickness_eta * lscl \n",
    "            # calculate Re_thickness\n",
    "            Re = (re * u_e * thickness)/mu_e\n",
    "            # calculate omega_thickness\n",
    "            omega = (2*np.pi*f*thickness)/u_e\n",
    "            \n",
    "            # make input u, rho, TwTe, Re, omega, mach,\n",
    "            point = []\n",
    "            point += list(u) \n",
    "            point += list(rho) \n",
    "            point += [TwTe, Re, omega, mach]\n",
    "            \n",
    "\n",
    "            res += [point] # add to batch\n",
    "        #res_m += [batch] \n",
    "    #res += [res_m]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## PREDICTIONS DATASET ##########################\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "########################## PREPPING FIRST MODE PREDICTIONS ##########################\n",
    "\n",
    "\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "preds_set = CustomDataset(res)\n",
    "preds_set = scaler2.fit_transform(preds_set)\n",
    "pred_loader = DataLoader(preds_set, batch_size=200, shuffle=False)\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in pred_loader:\n",
    "        outputs = model(batch.float())\n",
    "        predictions.extend(outputs.numpy()) \n",
    "\n",
    "predictions = torch.tensor(predictions)\n",
    "res = torch.tensor(res)\n",
    "sigs_inv = torch.tensor(sig_scaler[0].inverse_transform(predictions))\n",
    "\n",
    "res = []\n",
    "for ind in range(len(mach_dirs)):\n",
    "    res += [(sigs_inv[10000*ind:10000*(ind+1)])]\n",
    "\n",
    "final_res = []\n",
    "for rm in res:\n",
    "    ind = 0\n",
    "    s1 = []\n",
    "    for fi in range(len(freqs)):\n",
    "        s2 = []\n",
    "        for xi in range(len(freqs)):\n",
    "            s2 += [rm[ind]]\n",
    "            ind += 1\n",
    "        s1 += [s2]\n",
    "    final_res += [s1]\n",
    "res = torch.tensor(final_res)\n",
    "print(res.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## GRAPHING FIRST MODE PREDICTIONS ##########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tensor_np = res.numpy()\n",
    "\n",
    "# Create contour plots for each slice\n",
    "for i in range(tensor_np.shape[0]):\n",
    "    extent = [x_vals[0], x_vals[len(x_vals)-1], freqs[0], freqs[len(freqs)-1]]\n",
    "\n",
    "    plt.xlabel(\"x (m)\")\n",
    "    plt.ylabel(\"Frequency (f)\")\n",
    "    plt.title(\"Alpha Values, Mach \" + str(0.25*i+4))\n",
    "    plt.contourf(tensor_np[i], extent=extent)\n",
    "    plt.colorbar()\n",
    "    plt.savefig(\"first_mode_preds\\\\mach\" + str(0.25*i+4) + \".pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### FIRST MODE GROUND TRUTH #########################\n",
    "\n",
    "\n",
    "\n",
    "# Extract the positions of all important features from an example file\n",
    "pos = all_pos(\"data\\\\constantRe.tar\\\\constantRe\\\\fp_M4.0_Tw_300K\\\\lst\\\\second_mode\\\\case_f_00100000\\\\nfact.dat\")\n",
    "sigma_pos = pos[0]\n",
    "freq_pos = pos[1]\n",
    "Re_pos = pos[2]\n",
    "lscl_pos = pos[3]\n",
    "ue_pos = pos[4]\n",
    "mach = pos[5]\n",
    "x_pos = pos[6]\n",
    "\n",
    "\n",
    "root = \"data\\\\constantRe.tar\\\\constantRe\\\\\"\n",
    "mach_dirs = [l for l in os.listdir(root)] # should be 4\n",
    "\n",
    "mach_data = []\n",
    "\n",
    "for j in range(len(mach_dirs)):\n",
    "\n",
    "    m = j + 4\n",
    "    batch = []\n",
    "\n",
    "    # find the bl profile\n",
    "    _, bl_profile = bl_input_reader(root + mach_dirs[j] + \"\\\\mkBaseflow\\\\bl_input.dat\", tpose=True)\n",
    "\n",
    "    # We also want to figure out what the boundary thickness is. Since the length scale changes for each output in a frequency result,\n",
    "    # we want to extract the boundary layer thickness that is nondimensionalized by lscl, which is the value of eta at which u converges \n",
    "    # to about 1. I will determine this value by identifying the point at which the difference between two eta values is < 1e-9 or so.\n",
    "    eta = bl_profile[0]\n",
    "    u_fine = bl_profile[2] # fine-grained u values to get accurate reading on boundary layer thickness\n",
    "    ind = 0\n",
    "\n",
    "    while(ind < len(u_fine)-1 and abs(u_fine[ind] - u_fine[ind+1]) > 1e-9): ind += 1\n",
    "    thickness_eta = eta[ind]\n",
    "\n",
    "    # Grab ~60 equidistant points of u and rho information from the boundary layer itself. We only care about the boundary layer so\n",
    "    # ignore all points after 'ind'\n",
    "    \n",
    "    # to get exactly 60 points each time, first find how much buffer we need from the number of points on the boundary layer to get a \n",
    "    # number of points divisible by 60\n",
    "    diff = (ind+1)%60\n",
    "    # find where the new ind must be set; this number must be divisible by 60\n",
    "    new_ind = ind + diff + 1 if (ind+diff+1)%60 == 0 else ind + 1 - diff\n",
    "    step = round(new_ind/60) # just in case the boundary layer has less than 1 point?\n",
    "    \n",
    "    # Now find the relevant fmodes in this machdirectory\n",
    "    lst_root = root + mach_dirs[j] + \"\\\\lst\\\\first_mode\\\\\"\n",
    "    fmode_dirs = os.listdir(lst_root)\n",
    "    fmode_dirs = reduce(lambda a,h: a+[h] if h[:7]==\"case_f_\" else a, fmode_dirs, []) # all fmode directories\n",
    "    fmode = reduce(lambda a,h: a+[int(h[7:])] if h[:7]==\"case_f_\" else a, fmode_dirs, []) # all fmodes as ints\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for f in range(len(fmode_dirs)): # search through all of the frequency directories \n",
    "        _, ndata, mach = ndata_reader(lst_root + fmode_dirs[f]+\"\\\\nfact.dat\")\n",
    "        \n",
    "        # Look through each line to craft a datapoint and then place it in the list. However, we only want data points\n",
    "        # that give us x values from 0 to 1\n",
    "\n",
    "        for line in range(len(ndata[0]) if len(ndata)>0 else 0):\n",
    "            \n",
    "            sig = ndata[sigma_pos][line]\n",
    "            freq = ndata[freq_pos][line]\n",
    "            lscl = ndata[lscl_pos][line]\n",
    "            x = ndata[x_pos][line]\n",
    "\n",
    "            # Require a dimensional value for boundary layer thickness. To do this, we take the nondim eta value, eta = y/lscl,\n",
    "            # multiply it with lscl at this point, and have gotten a y value which defines the dim boundary layer thickness \n",
    "            thickness = thickness_eta * lscl\n",
    "\n",
    "            # Now we re-dim Re_l and dim omega with boundary layer thickness \n",
    "            sig = sig * thickness_eta # thickness_eta = y/lscl, so sig * lscl * y/lscl = sig * y\n",
    "            \n",
    "            # add this to the data\n",
    "            data += [(x, freq, sig)]\n",
    "\n",
    "    mach_data += [data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### GRAPH FIRST MODE GROUND TRUTH #########################\n",
    "\n",
    "\n",
    "\n",
    "stacked = reduce(lambda a,h: a + h, mach_data, [])\n",
    "_, _, z = zip(*stacked)\n",
    "vmin = min(z)\n",
    "vmax = max(z)\n",
    "\n",
    "m = 4\n",
    "for data in mach_data:\n",
    "    # Separate the data into x, y, and z components\n",
    "    x, y, z = zip(*data)\n",
    "\n",
    "    # Create a grid of x and y values\n",
    "    xi = np.linspace(min(x), max(x), 100)\n",
    "    yi = np.linspace(min(y), max(y), 100)\n",
    "    xi, yi = np.meshgrid(xi, yi)\n",
    "\n",
    "    # Interpolate the z values onto the grid\n",
    "    zi = griddata((x, y), z, (xi, yi), method='linear')\n",
    "\n",
    "    # Plot the contour plot\n",
    "    plt.figure()\n",
    "    cp = plt.contourf(xi, yi, zi, levels=14, cmap='viridis')\n",
    "    plt.colorbar(cp)\n",
    "    plt.xlabel('x (m)')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.title('Alpha, Mach ' + str(m))\n",
    "    plt.savefig(\"ground_truth\\\\first_mode\\\\mach\" + str(m) + \".pdf\")\n",
    "    plt.show()\n",
    "    m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                 | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model | ProfileCNN_LatentFCN | 48.3 K | train\n",
      "-------------------------------------------------------\n",
      "48.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "48.3 K    Total params\n",
      "0.193     Total estimated model params size (MB)\n",
      "c:\\Users\\fouad\\anaconda3\\envs\\gnn_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\fouad\\anaconda3\\envs\\gnn_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\fouad\\anaconda3\\envs\\gnn_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- SECOND MODE ---\n",
      "Train performance: 2.47%\n",
      "Test performance:  72.38%\n"
     ]
    }
   ],
   "source": [
    "######################### TRAIN SECOND MODE AMP PREDICTOR #########################\n",
    "\n",
    "\n",
    "\n",
    "model2, result = train_graph_classifier(model_name=\"AmplificationPredictor\",\n",
    "                                       train_loader = train_loader[1],\n",
    "                                       val_loader = val_loader[1],\n",
    "                                       test_loader = test_loader[1], \n",
    "                                       mode = 2)\n",
    "\n",
    "print(\" --- SECOND MODE ---\")\n",
    "print(\"Train performance: %4.2f%%\" % (100.0 * result[\"train\"]))\n",
    "print(\"Test performance:  %4.2f%%\" % (100.0 * result[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fouad\\AppData\\Local\\Temp\\ipykernel_26596\\197626645.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model2 = torch.load(\"sec_mode.pth\")\n"
     ]
    }
   ],
   "source": [
    "######################### SECOND MODE SAVE/LOAD #########################\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    if model2.training:\n",
    "        model2.eval()\n",
    "        print(\"Model set to eval mode.\")\n",
    "\n",
    "    torch.save(model2, \"sec_mode.pth\")\n",
    "except:\n",
    "    # To load:\n",
    "    model2 = torch.load(\"sec_mode.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### SECOND MODE PREDICTIONS #########################\n",
    "\n",
    "\n",
    "\n",
    "allBLs = \"allBLs\"\n",
    "root = \"allBLs\\\\\"\n",
    "\n",
    "mach_dirs = [l for l in os.listdir(root)]\n",
    "machs = [x*0.25 + 4 for x in range(13)]\n",
    "\n",
    "P0 = 341700.0\n",
    "gam = 1.4\n",
    "T1 = 63.9231\n",
    "R = 287.15\n",
    "Tw = 300.0\n",
    "\n",
    "# Now we want to loop through a list of frequencies and Reynolds numbers for each one of these profiles \n",
    "freqs = [x*5000 + 20000 for x in range(100)] # 20,000 - 200,000 f\n",
    "x_vals = [round(0.005*x + 0.001, 2) for x in range(100)] # 0.01 - 1.00 x\n",
    "\n",
    "x_vals = [round(0.02*x + 0.1, 3) for x in range(100)]\n",
    "freqs = [x*5000 + 20000 for x in range(100)]\n",
    "\n",
    "res = []\n",
    "\n",
    "for ind in range(len(mach_dirs)):\n",
    "    m = mach_dirs[ind]\n",
    "    mach = machs[ind]\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    # find the bl profile\n",
    "    _, bl_profile = bl_input_reader(root + m + \"\\\\bl_input.dat\", tpose=True)\n",
    "\n",
    "    # Grab T_w/T_e which is the first value of T\n",
    "    TwTe = bl_profile[4][0]\n",
    "    Te = Tw/TwTe\n",
    "    u_e = mach * np.sqrt(gam*R*Te)\n",
    "\n",
    "    # We also want to figure out what the boundary thickness is. Since the length scale changes for each output in a frequency result,\n",
    "    # we want to extract the boundary layer thickness that is nondimensionalized by lscl, which is the value of eta at which u converges \n",
    "    # to about 1. I will determine this value by identifying the point at which the difference between two eta values is < 1e-9 or so.\n",
    "    eta = bl_profile[0]\n",
    "    u_fine = bl_profile[2] # fine-grained u values to get accurate reading on boundary layer thickness\n",
    "    ind = 0\n",
    "\n",
    "    while(ind < len(u_fine)-1 and abs(u_fine[ind] - u_fine[ind+1]) > 1e-9): ind += 1\n",
    "    thickness_eta = eta[ind]\n",
    "\n",
    "    # Grab ~60 equidistant points of u and rho information from the boundary layer itself. We only care about the boundary layer so\n",
    "    # ignore all points after 'ind'\n",
    "    \n",
    "    # to get exactly 60 points each time, first find how much buffer we need from the number of points on the boundary layer to get a \n",
    "    # number of points divisible by 60\n",
    "    diff = (ind+1)%60\n",
    "    # find where the new ind must be set; this number must be divisible by 60\n",
    "    new_ind = ind + diff + 1 if (ind+diff+1)%60 == 0 else ind + 1 - diff\n",
    "    step = round(new_ind/60) # just in case the boundary layer has less than 1 point?\n",
    "    u = bl_profile[2][:new_ind:step]\n",
    "    rho = bl_profile[1][:new_ind:step]\n",
    "    \n",
    "\n",
    "    arg = 1+0.5*(gam-1.)*mach**2.\n",
    "    P1 = P0*(arg)**(-gam/(gam-1.))\n",
    "    re  = P1/(R*T1)\n",
    "    res_m = []\n",
    "    mu_e = 1.45151376745308e-06*Te**1.5e0/(Te+110.4e0)\n",
    "\n",
    "    for f in freqs:\n",
    "\n",
    "        batch = []\n",
    "        for x in x_vals:\n",
    "\n",
    "            # now we must determine our Re\n",
    "            # first determine the lscl\n",
    "            lscl = np.sqrt((mu_e*x)/(re*u_e))\n",
    "            # determine thickness\n",
    "            thickness = thickness_eta * lscl \n",
    "            # calculate Re_thickness\n",
    "            Re = (re * u_e * thickness)/mu_e\n",
    "            # calculate omega_thickness\n",
    "            omega = (2*np.pi*f*thickness)/u_e\n",
    "            \n",
    "            # make input u, rho, TwTe, Re, omega, mach,\n",
    "            point = []\n",
    "            point += list(u) \n",
    "            point += list(rho) \n",
    "            point += [TwTe, Re, omega, mach]\n",
    "            \n",
    "\n",
    "            res += [point] # add to batch\n",
    "    #res += [res_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "########################## PREPPING SECOND MODE PREDICTIONS ##########################\n",
    "\n",
    "\n",
    "\n",
    "scaler3 = StandardScaler()\n",
    "preds_set = CustomDataset(res)\n",
    "preds_set = scaler3.fit_transform(preds_set)\n",
    "pred_loader = DataLoader(preds_set, batch_size=200, shuffle=False)\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in pred_loader:\n",
    "        outputs = model2(batch.float())\n",
    "        predictions.extend(outputs.numpy()) \n",
    "\n",
    "predictions = torch.tensor(predictions)\n",
    "res = torch.tensor(res)\n",
    "sigs_inv = torch.tensor(sig_scaler[1].inverse_transform(predictions))\n",
    "\n",
    "res = []\n",
    "for ind in range(len(mach_dirs)):\n",
    "    res += [(sigs_inv[10000*ind:10000*(ind+1)])]\n",
    "\n",
    "final_res = []\n",
    "for rm in res:\n",
    "    ind = 0\n",
    "    s1 = []\n",
    "    for fi in range(len(freqs)):\n",
    "        s2 = []\n",
    "        for xi in range(len(freqs)):\n",
    "            s2 += [rm[ind]]\n",
    "            ind += 1\n",
    "        s1 += [s2]\n",
    "    final_res += [s1]\n",
    "res = torch.tensor(final_res)\n",
    "print(res.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## GRAPHING SECOND MODE PREDICTIONS ##########################\n",
    "\n",
    "\n",
    "\n",
    "vmin = res.min()\n",
    "vmax = res.max()\n",
    "\n",
    "tensor_np = res.numpy()\n",
    "\n",
    "# Create contour plots for each slice\n",
    "for i in range(tensor_np.shape[0]):\n",
    "    extent = [0, x_vals[len(x_vals)-1], freqs[0], freqs[len(freqs)-1]]\n",
    "\n",
    "    plt.xlabel(\"x (m)\")\n",
    "    plt.ylabel(\"Frequency (f)\")\n",
    "    plt.title(\"Alpha Values, Mach \" + str(0.25*i + 4))\n",
    "    plt.contourf(tensor_np[i], extent=extent)\n",
    "    plt.colorbar()\n",
    "    plt.savefig(\"sec_mode_preds\\\\mach\" + str(0.25*i + 4) + \".pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### SECOND MODE GROUND TRUTH #########################\n",
    "\n",
    "# Extract the positions of all important features from an example file\n",
    "pos = all_pos(\"data\\\\constantRe.tar\\\\constantRe\\\\fp_M4.0_Tw_300K\\\\lst\\\\second_mode\\\\case_f_00100000\\\\nfact.dat\")\n",
    "sigma_pos = pos[0]\n",
    "freq_pos = pos[1]\n",
    "Re_pos = pos[2]\n",
    "lscl_pos = pos[3]\n",
    "ue_pos = pos[4]\n",
    "mach = pos[5]\n",
    "x_pos = pos[6]\n",
    "\n",
    "\n",
    "root = \"data\\\\constantRe.tar\\\\constantRe\\\\\"\n",
    "mach_dirs = [l for l in os.listdir(root)] # should be 4\n",
    "\n",
    "mach_data = []\n",
    "\n",
    "for j in range(len(mach_dirs)):\n",
    "\n",
    "    m = ind + 4\n",
    "    batch = []\n",
    "\n",
    "    # find the bl profile\n",
    "    _, bl_profile = bl_input_reader(root + mach_dirs[j] + \"\\\\mkBaseflow\\\\bl_input.dat\", tpose=True)\n",
    "\n",
    "    # We also want to figure out what the boundary thickness is. Since the length scale changes for each output in a frequency result,\n",
    "    # we want to extract the boundary layer thickness that is nondimensionalized by lscl, which is the value of eta at which u converges \n",
    "    # to about 1. I will determine this value by identifying the point at which the difference between two eta values is < 1e-9 or so.\n",
    "    eta = bl_profile[0]\n",
    "    u_fine = bl_profile[2] # fine-grained u values to get accurate reading on boundary layer thickness\n",
    "    ind = 0\n",
    "\n",
    "    while(ind < len(u_fine)-1 and abs(u_fine[ind] - u_fine[ind+1]) > 1e-9): ind += 1\n",
    "    thickness_eta = eta[ind]\n",
    "\n",
    "    # Grab ~60 equidistant points of u and rho information from the boundary layer itself. We only care about the boundary layer so\n",
    "    # ignore all points after 'ind'\n",
    "    \n",
    "    # to get exactly 60 points each time, first find how much buffer we need from the number of points on the boundary layer to get a \n",
    "    # number of points divisible by 60\n",
    "    diff = (ind+1)%60\n",
    "    # find where the new ind must be set; this number must be divisible by 60\n",
    "    new_ind = ind + diff + 1 if (ind+diff+1)%60 == 0 else ind + 1 - diff\n",
    "    step = round(new_ind/60) # just in case the boundary layer has less than 1 point?\n",
    "    \n",
    "    # Now find the relevant fmodes in this machdirectory\n",
    "    lst_root = root + mach_dirs[j] + \"\\\\lst\\\\second_mode\\\\\"\n",
    "    fmode_dirs = os.listdir(lst_root)\n",
    "    fmode_dirs = reduce(lambda a,h: a+[h] if h[:7]==\"case_f_\" else a, fmode_dirs, []) # all fmode directories\n",
    "    fmode = reduce(lambda a,h: a+[int(h[7:])] if h[:7]==\"case_f_\" else a, fmode_dirs, []) # all fmodes as ints\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for f in range(len(fmode_dirs)): # search through all of the frequency directories \n",
    "        _, ndata, mach = ndata_reader(lst_root + fmode_dirs[f]+\"\\\\nfact.dat\")\n",
    "        \n",
    "        # Look through each line to craft a datapoint and then place it in the list. However, we only want data points\n",
    "        # that give us x values from 0 to 1\n",
    "\n",
    "        for line in range(len(ndata[0]) if len(ndata)>0 else 0):\n",
    "            \n",
    "            sig = ndata[sigma_pos][line]\n",
    "            freq = ndata[freq_pos][line]\n",
    "            lscl = ndata[lscl_pos][line]\n",
    "            x = ndata[x_pos][line]\n",
    "\n",
    "            # Require a dimensional value for boundary layer thickness. To do this, we take the nondim eta value, eta = y/lscl,\n",
    "            # multiply it with lscl at this point, and have gotten a y value which defines the dim boundary layer thickness \n",
    "            thickness = thickness_eta * lscl\n",
    "\n",
    "            # Now we re-dim Re_l and dim omega with boundary layer thickness \n",
    "            sig = sig * thickness_eta # thickness_eta = y/lscl, so sig * lscl * y/lscl = sig * y\n",
    "            \n",
    "            # add this to the data\n",
    "            data += [(x, freq, sig)]\n",
    "\n",
    "    mach_data += [data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### GRAPH SECOND MODE GROUND TRUTH #########################\n",
    "\n",
    "stacked = reduce(lambda a,h: a + h, mach_data, [])\n",
    "_, _, z = zip(*stacked) \n",
    "vmin = min(z)\n",
    "vmax = max(z)\n",
    "\n",
    "m = 4\n",
    "for data in mach_data:\n",
    "    # Separate the data into x, y, and z components\n",
    "    x, y, z = zip(*data)\n",
    "\n",
    "    # Create a grid of x and y values\n",
    "    xi = np.linspace(min(x), max(x), 100)\n",
    "    yi = np.linspace(min(y), max(y), 100)\n",
    "    xi, yi = np.meshgrid(xi, yi)\n",
    "\n",
    "    # Interpolate the z values onto the grid\n",
    "    zi = griddata((x, y), z, (xi, yi), method='linear')\n",
    "\n",
    "    # Plot the contour plot\n",
    "    plt.figure()\n",
    "    cp = plt.contourf(xi, yi, zi, levels=14, cmap='viridis')\n",
    "    plt.colorbar(cp)\n",
    "    plt.xlabel('x (m)')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.title('Alpha, Mach ' + str(m))\n",
    "    plt.savefig(\"ground_truth\\\\sec_mode\\\\mach\" + str(m) + \".pdf\")\n",
    "    plt.show()\n",
    "    m += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
